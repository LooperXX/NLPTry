{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "import numpy as np\nimport torch as t\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom torchtext.data import Field, Iterator, BucketIterator, TabularDataset\nfrom torchtext.vocab import Vectors\nstopwords_english \u003d stopwords.words(\u0027english\u0027)\nlemmatizer \u003d WordNetLemmatizer()\n# max_len \u003d 0\n# for i in train.examples:\n#     max_len \u003d max(max_len, len(vars(i)[\u0027Text\u0027]))\n# for i in val.examples:\n#     max_len \u003d max(max_len, len(vars(i)[\u0027Text\u0027]))\n# for i in test.examples:\n#     max_len \u003d max(max_len, len(vars(i)[\u0027Text\u0027]))\n# max_len"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "source": "data_path \u003d \u0027E:\\\\workspace\\\\python-workspace\\\\NLPTry\\\\2 - deep learning text classification\\\\\u0027\nvocab_path \u003d \u0027E:\\\\workspace\\\\jupyter_notebook\\\\.vector_cache\\\\\u0027\nclasses \u003d 5\nmax_len \u003d 56\nnum_filters \u003d 100\nkernel_sizes \u003d [3, 4, 5]\nlr\u003d0.001\nbatch_size\u003d32\nfreeze_embeddings \u003d True\ndrop_prob \u003d 0.3\nepochs \u003d 10\nprint_every \u003d 100\ndevice \u003d t.device(\u0027cuda:0\u0027)\nuse_gpu \u003d True",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "source": "def tokenize_en(text):\n    words \u003d word_tokenize(text)\n    return [lemmatizer.lemmatize(i) for i in words]",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "source": "TEXT \u003d Field(tokenize \u003d tokenize_en, \n            fix_length\u003dmax_len,stop_words\u003dstopwords_english,\n            lower \u003d True)\nLABEL \u003d Field(sequential\u003dFalse, use_vocab\u003dFalse)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "source": "train, val, test \u003d TabularDataset.splits(\n        path\u003ddata_path, train\u003d\u0027train.csv\u0027,skip_header\u003dTrue,\n        validation\u003d\u0027val.csv\u0027, test\u003d\u0027test.csv\u0027, format\u003d\u0027csv\u0027,\n        fields\u003d[(\u0027text\u0027, TEXT), (\u0027label\u0027, LABEL)])",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [],
      "source": "vector \u003d Vectors(\"glove.6B.100d.txt\", cache\u003dvocab_path)\n# vector.unk_init \u003d init.xavier_uniform\nTEXT.build_vocab(train, vectors\u003dvector)\nweight_matrix \u003d TEXT.vocab.vectors\nweight_matrix \u003d weight_matrix.cuda()",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "outputs": [],
      "source": "train_iter, val_iter \u003d BucketIterator.splits(\n        (train, val),\n        batch_sizes\u003d(batch_size, batch_size),\n        device\u003ddevice,\n        sort_key\u003dlambda x: len(x.Text), # the BucketIterator needs to be told what function it should use to group the data.\n        sort_within_batch\u003dFalse    \n)\ntest_iter \u003d Iterator(test, batch_size\u003dbatch_size, device\u003ddevice, sort\u003dFalse, sort_within_batch\u003dFalse, repeat\u003dFalse)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "outputs": [
        {
          "data": {
            "text/plain": "\n[torchtext.data.batch.Batch of size 32]\n\t[.text]:[torch.cuda.LongTensor of size 56x32 (GPU 0)]\n\t[.label]:[torch.cuda.LongTensor of size 32 (GPU 0)]"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 39
        }
      ],
      "source": "batch \u003d next(iter(train_iter))\nbatch",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "outputs": [],
      "source": "class BatchWrapper:\n    def __init__(self, dl, x_var, y_vars):\n        self.dl, self.x_var, self.y_vars \u003d dl, x_var, y_vars # we pass in the list of attributes for x and y\n    \n    def __iter__(self):\n        for batch in self.dl:\n            x \u003d getattr(batch, self.x_var) # we assume only one input in this wrapper\n            \n            if self.y_vars is not None: # we will concatenate y into a single tensor\n                y \u003d t.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim\u003d1).float()\n            else:\n                y \u003d t.zeros((1))\n\n            yield (x, y)\n    \n    def __len__(self):\n        return len(self.dl)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "outputs": [],
      "source": "train_dl \u003d BatchWrapper(train_iter, \"text\", [\"label\"])\nvalid_dl \u003d BatchWrapper(val_iter, \"text\", [\"label\"])\ntest_dl \u003d BatchWrapper(test_iter, \"text\", [\"label\"])\n# next(train_dl.__iter__())",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "outputs": [],
      "source": "class TextCNN(nn.Module):\n    def __init__(self, embeddings, vocab_size, embedding_dim, output_size, \n                 num_filters\u003d100, kernel_sizes\u003dNone , freeze_embeddings\u003dTrue, drop_prob\u003d0.3):\n        if kernel_sizes is None:\n\t        kernel_sizes \u003d [3, 4, 5]\n        super(TextCNN, self).__init__()\n        self.num_filters \u003d num_filters\n        self.embedding_dim \u003d embedding_dim\n        self.embedding \u003d nn.Embedding(vocab_size, embedding_dim)\n        self.embedding.weight \u003d nn.Parameter(embeddings) # all vectors\n        if freeze_embeddings:\n            self.embedding.requires_grad \u003d False\n        self.convs \u003d nn.ModuleList([\n            nn.Conv2d(1, num_filters, (k, embedding_dim)) \n            for k in kernel_sizes])\n        self.fc \u003d nn.Linear(len(kernel_sizes) * num_filters, output_size) \n        self.dropout \u003d nn.Dropout(drop_prob)\n        \n    def conv_and_pool(self, x, conv):\n        # squeeze last dim to get size: (batch_size, num_filters, conv_seq_length, 1) -\u003e (batch_size, num_filters, conv_seq_length)\n        x \u003d F.relu(conv(x)).squeeze()\n        # 1D pool over conv_seq_length, squeeze to get size: (batch_size, num_filters)\n        x_max \u003d F.max_pool1d(x, x.size(2)).squeeze(2)\n        return x_max\n\n    def forward(self, x):\n        # (batch_size, seq_length, embedding_dim)\n        embeddings \u003d self.embedding(x) \n        # embeddings.unsqueeze(1) creates a channel dimension that conv layers expect \n        # (batch_size, channel, seq_length, embedding_dim)\n        embeddings \u003d embeddings.unsqueeze(1)\n        conv_results \u003d [self.conv_and_pool(embeddings, conv) for conv in self.convs]\n        # concatenate results \n        x \u003d t.cat(conv_results, 1)\n        x \u003d self.dropout(x)\n        logits \u003d self.fc(x) \n        return F.softmax(logits, dim\u003d0)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "TextCNN(\n  (embedding): Embedding(15312, 100)\n  (convs): ModuleList(\n    (0): Conv2d(1, 100, kernel_size\u003d(3, 100), stride\u003d(1, 1))\n    (1): Conv2d(1, 100, kernel_size\u003d(4, 100), stride\u003d(1, 1))\n    (2): Conv2d(1, 100, kernel_size\u003d(5, 100), stride\u003d(1, 1))\n  )\n  (fc): Linear(in_features\u003d300, out_features\u003d5, bias\u003dTrue)\n  (dropout): Dropout(p\u003d0.3)\n)",
            "\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "model \u003d TextCNN(weight_matrix, weight_matrix.size(0), weight_matrix.size(1),classes, num_filters, kernel_sizes, freeze_embeddings, drop_prob)\nprint(model)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "outputs": [],
      "source": "criterion \u003d nn.CrossEntropyLoss()\noptimizer \u003d t.optim.Adam(model.parameters(), lr\u003dlr)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "outputs": [],
      "source": "# training loop\ndef train(model, train_loader, valid_loader, epochs, print_every\u003d100):\n    if use_gpu:\n        model.cuda()\n    counter \u003d 0\n    model.train()\n    for e in range(epochs):\n        # batch loop\n        for inputs, labels in train_loader:\n            counter +\u003d 1\n            if use_gpu:\n                inputs, labels \u003d inputs.cuda(), labels.cuda()\n            model.zero_grad()\n            output \u003d model(inputs)\n            loss \u003d criterion(output.squeeze(), labels.float())\n            loss.backward()\n            optimizer.step()\n            if counter % print_every \u003d\u003d 0:\n                val_losses \u003d []\n                accuracy \u003d []\n                model.eval()\n                for inputs, labels in valid_loader:\n                    if(use_gpu):\n                        inputs, labels \u003d inputs.cuda(), labels.cuda()\n                    output \u003d model(inputs)\n                    val_loss \u003d criterion(output.squeeze(), labels.float())\n                    val_losses.append(val_loss.item())\n                    predict_label \u003d np.argmax(output, axis\u003d0)\n                    accuracy.append(np.sum((predict_label \u003d\u003d labels) / float(batch_size)))\n                model.train()\n                print(\"Epoch\\t{}/{}...\".format(e+1, epochs),\n                      \"Step\\t}...\".format(counter),\n                      \"Loss\\t{:.6f}...\".format(loss.item()),\n                      \"Val_Loss\\t{:.6f}\".format(np.mean(val_losses)),\n                      \"Val_Accuracy\\t{:.6f}...\".format(accuracy))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "train(model, train_dl, valid_dl, epochs, print_every\u003dprint_every)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "66292",
            " ",
            "66292",
            "\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "test_losses \u003d []  # track loss\nnum_correct \u003d 0\nmodel.eval()\n# iterate over test data\nres \u003d np.empty([len(test_dl.dl.dataset), 2])\nindex \u003d 0\nfor inputs, labels in test_dl:\n    if use_gpu:\n        model.cuda()\n        inputs, labels \u003d inputs.cuda(), labels.cuda()\n    output \u003d model(inputs.transpose(1, 0))\n    predict_label \u003d np.argmax(output.detach().cpu(), axis\u003d1)\n    num \u003d len(predict_label)\n    res[index:index+num] \u003d t.stack((labels.squeeze().detach().cpu(), predict_label.float()), 0).transpose(1, 0)\n    index +\u003d num\nprint(index, len(test_dl.dl.dataset))\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "conda-root-py",
      "language": "python",
      "display_name": "Python [conda env:root] *"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}