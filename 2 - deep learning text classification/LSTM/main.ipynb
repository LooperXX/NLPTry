{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "import numpy as np\nimport torch as t\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom torchtext.data import Field, Iterator, BucketIterator, TabularDataset\nfrom torchtext.vocab import Vectors\nstopwords_english \u003d stopwords.words(\u0027english\u0027)\nlemmatizer \u003d WordNetLemmatizer()\n# max_len \u003d 0\n# for i in train.examples:\n#     max_len \u003d max(max_len, len(vars(i)[\u0027Text\u0027]))\n# for i in val.examples:\n#     max_len \u003d max(max_len, len(vars(i)[\u0027Text\u0027]))\n# for i in test.examples:\n#     max_len \u003d max(max_len, len(vars(i)[\u0027Text\u0027]))\n# max_len"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "source": "data_path \u003d \u0027E:\\\\workspace\\\\python-workspace\\\\NLPTry\\\\2 - deep learning text classification\\\\\u0027\nvocab_path \u003d \u0027E:\\\\workspace\\\\jupyter_notebook\\\\.vector_cache\\\\\u0027\nclasses \u003d 5\nmax_len \u003d 56\nhidden_dim \u003d 100\nlr\u003d0.001\nbatch_size\u003d32\nepochs \u003d 10\nprint_every \u003d 100\ndevice \u003d t.device(\u0027cuda:0\u0027)\nuse_gpu \u003d True",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "source": "def tokenize_en(text):\n    words \u003d word_tokenize(text)\n    return [lemmatizer.lemmatize(i) for i in words]",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "source": "TEXT \u003d Field(tokenize \u003d tokenize_en, \n            fix_length\u003dmax_len,stop_words\u003dstopwords_english,\n            lower \u003d True)\nLABEL \u003d Field(sequential\u003dFalse, use_vocab\u003dFalse)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "source": "train, val, test \u003d TabularDataset.splits(\n        path\u003ddata_path, train\u003d\u0027train.csv\u0027,skip_header\u003dTrue,\n        validation\u003d\u0027val.csv\u0027, test\u003d\u0027test.csv\u0027, format\u003d\u0027csv\u0027,\n        fields\u003d[(\u0027text\u0027, TEXT), (\u0027label\u0027, LABEL)])",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "source": "vector \u003d Vectors(\"glove.6B.100d.txt\", cache\u003dvocab_path)\n# vector.unk_init \u003d init.xavier_uniform\nTEXT.build_vocab(train, vectors\u003dvector)\nweight_matrix \u003d TEXT.vocab.vectors\nweight_matrix \u003d weight_matrix.cuda()",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "source": "train_iter, val_iter \u003d BucketIterator.splits(\n        (train, val),\n        batch_sizes\u003d(batch_size, batch_size),\n        device\u003ddevice,\n        sort_key\u003dlambda x: len(x.Text), # the BucketIterator needs to be told what function it should use to group the data.\n        sort_within_batch\u003dFalse    \n)\ntest_iter \u003d Iterator(test, batch_size\u003dbatch_size, device\u003ddevice, sort\u003dFalse, sort_within_batch\u003dFalse, repeat\u003dFalse)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [
        {
          "data": {
            "text/plain": "\n[torchtext.data.batch.Batch of size 32]\n\t[.text]:[torch.cuda.LongTensor of size 56x32 (GPU 0)]\n\t[.label]:[torch.cuda.LongTensor of size 32 (GPU 0)]"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 14
        }
      ],
      "source": "batch \u003d next(iter(train_iter))\nbatch",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "source": "class BatchWrapper:\n    def __init__(self, dl, x_var, y_vars):\n        self.dl, self.x_var, self.y_vars \u003d dl, x_var, y_vars # we pass in the list of attributes for x and y\n    \n    def __iter__(self):\n        for batch in self.dl:\n            x \u003d getattr(batch, self.x_var) # we assume only one input in this wrapper\n            \n            if self.y_vars is not None: # we will concatenate y into a single tensor\n                y \u003d t.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim\u003d1).float()\n            else:\n                y \u003d t.zeros((1))\n\n            yield (x, y)\n    \n    def __len__(self):\n        return len(self.dl)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "source": "train_dl \u003d BatchWrapper(train_iter, \"text\", [\"label\"])\nvalid_dl \u003d BatchWrapper(val_iter, \"text\", [\"label\"])\ntest_dl \u003d BatchWrapper(test_iter, \"text\", [\"label\"])\n# next(train_dl.__iter__())",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "source": "class LSTMBase(nn.Module):\n    def __init__(self, embeddings, vocab_size, embedding_dim, classes, hidden_dim, batch_size, num_layers\u003d2, freeze_embeddings\u003dTrue, prob\u003d0.2):\n        super(LSTMBase, self).__init__()\n        self.embedding_dim \u003d embedding_dim\n        self.hidden_dim \u003d hidden_dim\n        self.batch_size \u003d batch_size\n        self.num_layers \u003d num_layers\n        self.embedding \u003d nn.Embedding(vocab_size, embedding_dim)\n        self.embedding.weight \u003d nn.Parameter(embeddings, requires_grad\u003dfreeze_embeddings)\n        self.lstm \u003d nn.LSTM(embedding_dim, self.hidden_dim, num_layers\u003dnum_layers, dropout\u003dprob, batch_first\u003dTrue)\n        self.dropout \u003d nn.Dropout(0.3)\n        self.fc \u003d nn.Linear(self.hidden_dim, classes)\n        self.init \u003d self.init_hidden()\n\n    def init_hidden(self, batch_size\u003dNone):\n        if batch_size \u003d\u003d None:\n            batch_size \u003d self.batch_size\n        h0 \u003d t.zeros((self.num_layers, batch_size, self.hidden_dim))\n        c0 \u003d t.zeros((self.num_layers, batch_size, self.hidden_dim))\n        if use_gpu:\n            h0 \u003d h0.cuda()\n            c0 \u003d c0.cuda()\n        return h0, c0\n\n    def forward(self, input):\n        # (batch_size, seq_length, embedding_dim)\n        embeddings \u003d self.embedding(input)\n        h, c \u003d self.init_hidden(embeddings.size()[0])\n        out, (h, c) \u003d self.lstm(embeddings, (h, c))\n        out \u003d self.dropout(out[-1])\n        out \u003d self.fc(out)\n        return F.softmax(out, dim\u003d0)\n    ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "TextCNN(\n  (embedding): Embedding(15312, 100)\n  (convs): ModuleList(\n    (0): Conv2d(1, 100, kernel_size\u003d(3, 100), stride\u003d(1, 1))\n    (1): Conv2d(1, 100, kernel_size\u003d(4, 100), stride\u003d(1, 1))\n    (2): Conv2d(1, 100, kernel_size\u003d(5, 100), stride\u003d(1, 1))\n  )\n  (fc): Linear(in_features\u003d300, out_features\u003d5, bias\u003dTrue)\n  (dropout): Dropout(p\u003d0.5)\n  (softmax): Softmax()\n)",
            "\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "model \u003d LSTMBase(weight_matrix, weight_matrix.size(0), weight_matrix.size(1), classes, hidden_dim, batch_size, num_layers, freeze_embeddings)\nprint(model)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "source": "criterion \u003d nn.CrossEntropyLoss()\noptimizer \u003d t.optim.Adam(model.parameters(), lr\u003dlr)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "source": "# training loop\ndef train(model, train_loader, valid_loader, epochs, print_every\u003d100):\n    if use_gpu:\n        model.cuda()\n    counter \u003d 0\n    model.train()\n    for e in range(epochs):\n        # batch loop\n        for inputs, labels in train_loader:\n            counter +\u003d 1\n            if use_gpu:\n                inputs, labels \u003d inputs.cuda(), labels.cuda()\n            model.zero_grad()\n            output \u003d model(inputs)\n            loss \u003d criterion(output.squeeze(), labels.float())\n            loss.backward()\n            optimizer.step()\n            if counter % print_every \u003d\u003d 0:\n                val_losses \u003d []\n                accuracy \u003d []\n                model.eval()\n                for inputs, labels in valid_loader:\n                    if(use_gpu):\n                        inputs, labels \u003d inputs.cuda(), labels.cuda()\n                    output \u003d model(inputs)\n                    val_loss \u003d criterion(output.squeeze(), labels.float())\n                    val_losses.append(val_loss.item())\n                    predict_label \u003d np.argmax(output, axis\u003d0)\n                    accuracy.append(np.sum((predict_label \u003d\u003d labels) / float(batch_size)))\n                model.train()\n                print(\"Epoch\\t{}/{}...\".format(e+1, epochs),\n                      \"Step\\t}...\".format(counter),\n                      \"Loss\\t{:.6f}...\".format(loss.item()),\n                      \"Val_Loss\\t{:.6f}\".format(np.mean(val_losses)),\n                      \"Val_Accuracy\\t{:.6f}...\".format(accuracy))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "train(model, train_dl, valid_dl, epochs, print_every\u003dprint_every)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [
        {
          "name": "stderr",
          "text": [
            "F:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim\u003dX as an argument.\n"
          ],
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": [
            "66292",
            " ",
            "66292",
            "\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "test_losses \u003d []  # track loss\nnum_correct \u003d 0\nmodel.eval()\n# iterate over test data\nres \u003d np.empty([len(test_dl.dl.dataset), 2])\nindex \u003d 0\nfor inputs, labels in test_dl:\n    if use_gpu:\n        model.cuda()\n        inputs, labels \u003d inputs.cuda(), labels.cuda()\n    output \u003d model(inputs.transpose(1, 0))\n    predict_label \u003d np.argmax(output.detach().cpu(), axis\u003d1)\n    num \u003d len(predict_label)\n    res[index:index+num] \u003d t.stack((labels.squeeze().detach().cpu(), predict_label.float()), 0).transpose(1, 0)\n    index +\u003d num\n# print(index, len(test_dl.dl.dataset))\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "conda-root-py",
      "language": "python",
      "display_name": "Python [conda env:root] *"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}